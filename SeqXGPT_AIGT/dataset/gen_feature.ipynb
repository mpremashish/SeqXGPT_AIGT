{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e134d7b-d6f0-497c-9ab1-d9a0ad3cf8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import httpx\n",
    "import msgpack\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e291521-bc02-47cf-bd71-f93bc57d41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from jsonlines) (21.4.0)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49188de8-c6bd-4141-961a-614edd0a9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_2_api = 'http://0.0.0.0:6006/inference'\n",
    "gpt_neo_api = 'http://0.0.0.0:6007/inference'\n",
    "gpt_J_api = 'http://0.0.0.0:6008/inference'\n",
    "llama_api = 'http://0.0.0.0:6009/inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ae99f7-cb71-4017-9fad-b428b2467139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "def merge_jsonl(input_files, output_file):\n",
    "    combined_data = []\n",
    "    for file in input_files:\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                json_data = json.loads(line.strip())\n",
    "                combined_data.append(json_data)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in combined_data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_files = glob.glob('./SeqXGPT-Bench/*.jsonl')  # List of JSONL files in current directory\n",
    "output_file = 'input.jsonl'\n",
    "\n",
    "merge_jsonl(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d6a860-951b-42de-ad1c-b007126ed716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_api(text, api_url, do_generate=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param text: input text\n",
    "    :param api_url: api\n",
    "    :param do_generate: whether generate or not\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with httpx.Client(timeout=None) as client:\n",
    "        post_data = {\n",
    "            \"text\": text,\n",
    "            \"do_generate\": do_generate,\n",
    "        }\n",
    "        prediction = client.post(api_url,\n",
    "                                 data=msgpack.packb(post_data),\n",
    "                                 timeout=None)\n",
    "    if prediction.status_code == 200:\n",
    "        content = msgpack.unpackb(prediction.content)\n",
    "    else:\n",
    "        content = None\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8e91b2-63db-4bc4-9d4a-aba235ea0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input_file, output_file, model_names, model_apis):\n",
    "    \"\"\"\n",
    "    get [losses, begin_idx_list, ll_tokens_list, label_int, label] based on raw lines\n",
    "    \"\"\"\n",
    "\n",
    "    en_labels = {\n",
    "        'gpt2': 0,\n",
    "        'gptneo': 1,\n",
    "        'gptj': 1,\n",
    "        'llama': 2,\n",
    "        'gpt3re': 3,\n",
    "        'gpt3sum': 3,\n",
    "        'human': 4,\n",
    "        'alpaca': None,\n",
    "        'dolly': None,\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = [json.loads(line) for line in f]\n",
    "\n",
    "    print('input file:{}, length:{}'.format(input_file, len(lines)))\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for data in tqdm(lines):\n",
    "            line = data['text']\n",
    "            label = data['label']\n",
    "\n",
    "            losses = data['losses'] if 'losses' in data else []\n",
    "            begin_idx_list = data['begin_idx_list'] if 'begin_idx_list' in data else []\n",
    "            ll_tokens_list = data['ll_tokens_list'] if 'll_tokens_list' in data else []\n",
    "            \n",
    "            label_dict = en_labels\n",
    "\n",
    "            label_int = label_dict[label]\n",
    "\n",
    "            error_flag = False\n",
    "            for api in model_apis:\n",
    "                try:\n",
    "                    loss, begin_word_idx, ll_tokens = access_api(line, api)\n",
    "                except TypeError:\n",
    "                    print(\"return NoneType, probably gpu OOM, discard this sample\")\n",
    "                    error_flag = True\n",
    "                    break\n",
    "                losses.append(loss)\n",
    "                begin_idx_list.append(begin_word_idx)\n",
    "                ll_tokens_list.append(ll_tokens)\n",
    "            # if oom, discard this sample\n",
    "            if error_flag:\n",
    "                continue\n",
    "\n",
    "            result = {\n",
    "                'losses': losses,\n",
    "                'begin_idx_list': begin_idx_list,\n",
    "                'll_tokens_list': ll_tokens_list,\n",
    "                'label_int': label_int,\n",
    "                'label': label,\n",
    "                'text': line\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e34cb09-755b-4be6-bd37-c4131a44937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file:small_input.jsonl, length:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:12<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "get_features('input.jsonl','output_all.jsonl',['gpt_2', 'gpt_neo'],[gpt_2_api,gpt_neo_api])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e7d5f-8af5-4b82-a9d2-67e2557bf15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features('output_all.jsonl','output_all.jsonl',['gpt_J', 'llama'],[gpt_J_api,llama_api])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e114c1-35ee-4e94-88aa-b83e16b98fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "input_jsonl = read_jsonl('input.jsonl')\n",
    "output_jsonl = read_jsonl('output_all.jsonl')\n",
    "\n",
    "for item1, item2 in zip(input_jsonl, output_jsonl):\n",
    "    if 'prompt_len' in item1:\n",
    "        item2['prompt_len'] = item1['prompt_len']\n",
    "    else:\n",
    "        item2['prompt_len'] = 0\n",
    "\n",
    "write_jsonl('output_ prompt_merged.jsonl',output_jsonl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2853fb7-f700-4061-a3e3-461c8133a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data binary\n",
    "import jsonlines\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "binary_data = read_jsonl('output_ prompt_merged.jsonl')\n",
    "\n",
    "for item in binary_data:\n",
    "    if item['label'] != 'human':\n",
    "        item['label'] = 'AI'\n",
    "\n",
    "write_jsonl('output_binary_merged.jsonl',binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4023a36-e545-4a41-87f6-d400f553a857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
